---
# title for above
title: "Data Exchange Roy Schwartz Efficient Nlp"
date: 2022-12-02T16:27:14+01:00
draft: false
rating: 4
identifyer:
  - url: https://thedataexchange.media/efficient-methods-for-natural-language-processing/
  - arxiv: https://arxiv.org/abs/2209.00099
RSS:
  - url: "https://dts.podtrac.com/redirect.mp3/www.buzzsprout.com/682433/11751438-efficient-methods-for-natural-language-processing.mp3"
  - length: "32921056"
  - duration: 2740
  - image: ""
  - title: Efficient Methods for Natural Language Processing
  - link: https://thedataexchange.media/efficient-methods-for-natural-language-processing/
# full title of resource
link_title: "Efficient Methods for Natural Language Processing"
pubDate: "december 1, 2022"
authors:
  - Roy Schwartz
  - Ben Lorica
difficulties:
  - advanced
linktypes:
    - paper
    - podcastepisode
tags:
    - transformers
    - LLM
    - training pipelines
---

## Efficient Methods for Natural Language Processing
Is a podcast interview with Roy Schwartz and can be found here](https://thedataexchange.media/efficient-methods-for-natural-language-processing/) or in your podcast

> Roy Schwartz is Professor of Natural Language Processing at The Hebrew University of Jerusalem. We discussed a recent survey paper (co-written by Roy) that presents a broad overview of existing methods to improve NLP efficiency through the lens of traditional NLP pipelines. Our conversation covered the following key areas:

    Data: Using fewer training instances or better utilizing those available can increase efficiency.
    Model Design
    Training:  pre-training and fine tuning.
    Inference and compression


## what do I think about it
I was really surprised by the efficient data use steps. It is possible to maximize training efficiency by training on the most difficult samples. This is sort of related to [creating subâ€“samples using a maximum dissimilarity approach](https://topepo.github.io/caret/data-splitting.html#splitting-based-on-the-predictors)

