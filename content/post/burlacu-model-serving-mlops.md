---
# title for above
title: "Burlacu Model Serving Mlops"
date: 2022-12-04T11:48:57+01:00
draft: false
rating: 3
identifyer:
  - url: https://alexandruburlacu.github.io/posts/2022-09-25-neptuneai-ml-serving
# full title of resource
link_title: "How to Solve the Model Serving Component of the MLOps Stack"
pubDate: "sep 24, 2022"
authors:
  - Alexandru Burlacu 
difficulties:
  - advanced
linktypes:
    - blogpost
tags:
    - model serving
    - mlops
    - 

---

## How to Solve the Model Serving Component of the MLOps Stack
Is a paper about serving models and can be found [here](https://alexandruburlacu.github.io/posts/2022-09-25-neptuneai-ml-serving).

> When people talk about productionizing ML models, they use the term serving rather than simply deployment. So what does this mean?
To serve a model is to expose it to the real world and ensure it meets all your production requirements, aka your latency, accuracy, fault-tolerance, and throughput are all at the “business is happy” level. 

Shows a basic setup, intermediate and advanced setup.

## what do I think about it
Great article about model serving! Shows many of the tradeoffs and gives practical advice.
